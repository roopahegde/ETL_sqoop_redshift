{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL CASESTUDY\n",
    "\n",
    "<b>Problem Statement</b>\n",
    "\n",
    "\n",
    "<u>Extract</u> data of atm transactions form RDS and perform ETL operations to <u>load</u> data into Amazon Redshift warehouse, using Sqoop pipeline, HDFS and S3 storages and PySpark for processing and <u>transformation</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "* [Extract Data](#extract) \n",
    "* [Transform Data](#transform)\n",
    "    * [Dimension tables](#dim)\n",
    "        * [Location](#loc)\n",
    "        * [ATM](#atm)\n",
    "        * [Date](#date)\n",
    "        * [Card](#card)\n",
    "    * [Fact table](#fact)\n",
    "* [Load Data into S3](#load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting environment variables\n",
    "\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_161/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spark session\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-0-211.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ATM_Transactions</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=ATM_Transactions>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get spark context\n",
    "spark = SparkSession.builder.appName('ATM_Transactions').master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Data <a id=\"extract\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing StructType\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, FloatType, DecimalType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The Datatypes of `atm_lat`,`atm_lon`,`weather_lat`,`weather_lon`,`temp` and `rain_3h` has been mentioned has String\n",
    " insted of float or double or decimal because when reading the parquet file with custom schema, pyspark was not reading\n",
    " these columns as numeric. Efforts to specify it as any numeric type was throwing as error mentioned below.\n",
    "\n",
    "<u> java.lang.UnsupportedOperationException: parquet.column.values.dictionary.PlainValuesDictionary$PlainBinaryDictionary</u>\n",
    "\n",
    " One of the work arounds is to read these columns as string type and then convert the column type to a numeric type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema for the parquet file\n",
    "\n",
    "fileSchema = StructType([StructField('year', IntegerType(),True),\n",
    "                        StructField('month', StringType(),True),\n",
    "                        StructField('day', IntegerType(),True),\n",
    "                        StructField('weekday', StringType(),True),\n",
    "                        StructField('hour', IntegerType(),True),\n",
    "                        StructField('atm_status', StringType(),True),\n",
    "                        StructField('atm_id', StringType(),True),\n",
    "                        StructField('atm_manufacturer', StringType(),True),\n",
    "                        StructField('atm_location', StringType(),True),\n",
    "                        StructField('atm_streetname', StringType(),True),\n",
    "                        StructField('atm_street_number', IntegerType(),True),\n",
    "                        StructField('atm_zipcode', IntegerType(),True),\n",
    "                        StructField('atm_lat', StringType(),True),\n",
    "                        StructField('atm_lon', StringType(),True),\n",
    "                        StructField('currency', StringType(),True),\n",
    "                        StructField('card_type', StringType(),True),\n",
    "                        StructField('transaction_amount', IntegerType(),True),\n",
    "                        StructField('service', StringType(),True),\n",
    "                        StructField('message_code', StringType(),True),\n",
    "                        StructField('message_text', StringType(),True),\n",
    "                        StructField('weather_lat', StringType(),True),\n",
    "                        StructField('weather_lon', StringType(),True),\n",
    "                        StructField('weather_city_id', IntegerType(),True),\n",
    "                        StructField('weather_city_name', StringType(),True),\n",
    "                        StructField('temp', StringType(),True),\n",
    "                        StructField('pressure', IntegerType(),True),\n",
    "                        StructField('humidity', IntegerType(),True),\n",
    "                        StructField('wind_speed', IntegerType(),True),\n",
    "                        StructField('wind_deg', IntegerType(),True),\n",
    "                        StructField('rain_3h', StringType(),True),\n",
    "                        StructField('clouds_all', IntegerType(),True),\n",
    "                        StructField('weather_id', IntegerType(),True),\n",
    "                        StructField('weather_main', StringType(),True),\n",
    "                        StructField('weather_description', StringType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the parquet file from HDFS\n",
    "master_df = spark.read.schema(fileSchema).load(\"atm_trans_data/99cabc80-c250-4b0e-97f2-1c6766ab412e.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- atm_id: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: string (nullable = true)\n",
      " |-- atm_lon: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- weather_lat: string (nullable = true)\n",
      " |-- weather_lon: string (nullable = true)\n",
      " |-- weather_city_id: integer (nullable = true)\n",
      " |-- weather_city_name: string (nullable = true)\n",
      " |-- temp: string (nullable = true)\n",
      " |-- pressure: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- wind_speed: integer (nullable = true)\n",
      " |-- wind_deg: integer (nullable = true)\n",
      " |-- rain_3h: string (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the schema\n",
    "master_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data <a id=\"transform\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few numeric values in 'weather_main' column, here I am replacing the numeric values with null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the numeric values with null\n",
    "from pyspark.sql.functions import col , column, when\n",
    "master_df = master_df.withColumn(\"weather_main\", when(col(\"weather_main\").cast(\"int\").isNotNull(), None)\n",
    "                                     .otherwise(col(\"weather_main\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- atm_id: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: decimal(10,3) (nullable = true)\n",
      " |-- atm_lon: decimal(10,3) (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- weather_lat: decimal(10,3) (nullable = true)\n",
      " |-- weather_lon: decimal(10,3) (nullable = true)\n",
      " |-- weather_city_id: integer (nullable = true)\n",
      " |-- weather_city_name: string (nullable = true)\n",
      " |-- temp: decimal(10,3) (nullable = true)\n",
      " |-- pressure: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- wind_speed: integer (nullable = true)\n",
      " |-- wind_deg: integer (nullable = true)\n",
      " |-- rain_3h: decimal(10,3) (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Change the datatype of \"atm_lat\",\"atm_lon\",\"weather_lat\",\"weather_lon\",\"temp\",\"rain_3h\" to double.\n",
    "\n",
    "\n",
    "master_df = master_df.withColumn('atm_lat', col(\"atm_lat\").cast(\"Decimal(10,3)\"))\\\n",
    ".withColumn('atm_lon', col(\"atm_lon\").cast(\"Decimal(10,3)\"))\\\n",
    ".withColumn('weather_lat', col(\"weather_lat\").cast(\"Decimal(10,3)\"))\\\n",
    ".withColumn('weather_lon', col(\"weather_lon\").cast(\"Decimal(10,3)\"))\\\n",
    ".withColumn('temp', col(\"temp\").cast(\"Decimal(10,3)\"))\\\n",
    ".withColumn('rain_3h', col(\"rain_3h\").cast(\"Decimal(10,3)\"))\n",
    "\n",
    "master_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of total number of records read from HDFS\n",
    "master_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension tables(DataFrames) <a id=\"dim\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been extracted and needs to be transformed further into fact and dimension tables as a preparation to load into data warehouse and perform analytical queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location<a id=\"loc\"></a>\n",
    "\n",
    "- DIM_LOCATION"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Column Name   Data Type\n",
    "-----------   ---------\n",
    "location_id  | INT\n",
    "location     | VARCHAR(50)\n",
    "streetname   | VARCHAR(255)\n",
    "street_number| INT\n",
    "zipcode      | INT\n",
    "lat          | DECIMAL(10,3)\n",
    "lon          | DECIMAL(10,3)\n",
    "\n",
    "\n",
    "PRIMARY KEY location_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+-----------------+-----------+-------+-------+\n",
      "| atm_location|   atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|\n",
      "+-------------+-----------------+-----------------+-----------+-------+-------+\n",
      "|      Gistrup|       Hadsundvej|              346|       9260| 56.997|  9.993|\n",
      "|Intern Skagen|Sct. Laurentiivej|               36|       9990| 57.723| 10.590|\n",
      "|      KÃƒÂ¸ge|   SÃƒÂ¸ndre Alle|                1|       4600| 55.454| 12.181|\n",
      "|    Svenstrup|   GodthÃƒÂ¥bsvej|               14|       9230| 56.973|  9.851|\n",
      "|  Aalborg Syd|         Hobrovej|              440|       9200| 57.005|  9.881|\n",
      "+-------------+-----------------+-----------------+-----------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating location dimension table\n",
    "DIM_LOCATION = master_df.select('atm_location', 'atm_streetname', 'atm_street_number', 'atm_zipcode', 'atm_lat', 'atm_lon').distinct()\n",
    "DIM_LOCATION.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-----------------+-----------+-------+-------+-----------+\n",
      "|        atm_location|  atm_streetname|atm_street_number|atm_zipcode|atm_lat|atm_lon|location_id|\n",
      "+--------------------+----------------+-----------------+-----------+-------+-------+-----------+\n",
      "|        KÃƒÂ¸benhavn|  Regnbuepladsen|                5|       1550| 55.676| 12.571|          1|\n",
      "|Intern  KÃƒÂ¸benhavn|RÃƒÂ¥dhuspladsen|               75|       1550| 55.676| 12.571|          2|\n",
      "|       Frederiksberg| Gammel Kongevej|              157|       1850| 55.677| 12.537|          3|\n",
      "|              Lyngby|     Jernbanevej|                6|       2800| 55.772| 12.500|          4|\n",
      "|        HelsingÃƒÂ¸r|  Sct. Olai Gade|               39|       3000| 56.036| 12.612|          5|\n",
      "|         HillerÃƒÂ¸d|KÃƒÂ¸benhavnsvej|               31|       3400| 55.933| 12.314|          6|\n",
      "|HillerÃƒÂ¸d IdrÃƒ...|      Milnersvej|               39|       3400| 55.921| 12.299|          7|\n",
      "|          Svogerslev|    BrÃƒÂ¸nsager|                1|       4000| 55.634| 12.018|          8|\n",
      "|            Roskilde|KÃƒÂ¸benhavnsvej|               65|       4000| 55.642| 12.106|          9|\n",
      "|    Intern  Roskilde|KÃƒÂ¸benhavnsvej|               65|       4000| 55.642| 12.106|         10|\n",
      "+--------------------+----------------+-----------------+-----------+-------+-------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding primary key\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "# Create the window specification\n",
    "w = Window.orderBy(\"atm_zipcode\")\n",
    "\n",
    "# Use row number with the window specification\n",
    "DIM_LOCATION = DIM_LOCATION.withColumn(\"location_id\", func.row_number().over(w))\n",
    "\n",
    "DIM_LOCATION.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['location',\n",
       " 'streetname',\n",
       " 'street_number',\n",
       " 'zipcode',\n",
       " 'lat',\n",
       " 'lon',\n",
       " 'location_id']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming columns according to target schema\n",
    "DIM_LOCATION = DIM_LOCATION.withColumnRenamed(\"location_id\",\"location_id\") \\\n",
    ".withColumnRenamed(\"atm_location\",\"location\") \\\n",
    ".withColumnRenamed(\"atm_streetname\",\"streetname\")\\\n",
    ".withColumnRenamed(\"atm_street_number\",\"street_number\")\\\n",
    ".withColumnRenamed(\"atm_zipcode\",\"zipcode\")\\\n",
    ".withColumnRenamed(\"atm_lat\",\"lat\")\\\n",
    ".withColumnRenamed(\"atm_lon\",\"lon\")\n",
    "\n",
    "DIM_LOCATION.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- location_id: integer (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- streetname: string (nullable = true)\n",
      " |-- street_number: integer (nullable = true)\n",
      " |-- zipcode: integer (nullable = true)\n",
      " |-- lat: decimal(10,3) (nullable = true)\n",
      " |-- lon: decimal(10,3) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rearranging columns according to target schema\n",
    "\n",
    "DIM_LOCATION = DIM_LOCATION.select('location_id', 'location', 'streetname', 'street_number', 'zipcode', 'lat', 'lon')\n",
    "\n",
    "DIM_LOCATION.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying the count\n",
    "DIM_LOCATION.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATM <a id=\"atm\"></a>\n",
    "\n",
    "- DIM_ATM"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Column Name      Data Type\n",
    "-----------      ---------\n",
    "atm_id            | INT\n",
    "atm_number        | VARCHAR(20)\n",
    "atm_manufacturer  | VARCHAR(255)\n",
    "atm_location_id   | INT\n",
    "\n",
    "- PRIMARY KEY atm_id\n",
    "- FOREIGN KEY atm_location_id REFERENCES DIM_LOCATION (location_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+-------+-------+\n",
      "|atm_id|atm_manufacturer|atm_lat|atm_lon|\n",
      "+------+----------------+-------+-------+\n",
      "|     1|             NCR| 55.233| 11.763|\n",
      "|     2|             NCR| 57.043|  9.950|\n",
      "|     2|             NCR| 57.043|  9.950|\n",
      "|     3|             NCR| 56.139|  9.154|\n",
      "|     4|             NCR| 55.634| 12.018|\n",
      "+------+----------------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get ATM dimension data\n",
    "ATM = master_df.select('atm_id', 'atm_manufacturer', 'atm_lat', 'atm_lon')\n",
    "ATM.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns to facilitate merging with location data\n",
    "ATM = ATM.withColumnRenamed(\"atm_id\",\"atm_number\") \\\n",
    ".withColumnRenamed(\"atm_manufacturer\",\"atm_manufacturer\")\\\n",
    ".withColumnRenamed(\"atm_lat\",\"lat\")\\\n",
    ".withColumnRenamed(\"atm_lon\",\"lon\")\n",
    "\n",
    "#Validate\n",
    "ATM.schema.names\n",
    "\n",
    "\n",
    "# Merge with location data\n",
    "DIM_ATM = ATM.join(DIM_LOCATION, on=[\"lat\", \"lon\"], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----------------+-----------+------------+--------------+-------------+-------+\n",
      "|   lat|   lon|atm_number|atm_manufacturer|location_id|    location|    streetname|street_number|zipcode|\n",
      "+------+------+----------+----------------+-----------+------------+--------------+-------------+-------+\n",
      "|55.676|12.571|        85| Diebold Nixdorf|          1|KÃƒÂ¸benhavn|Regnbuepladsen|            5|   1550|\n",
      "|55.676|12.571|        85| Diebold Nixdorf|          1|KÃƒÂ¸benhavn|Regnbuepladsen|            5|   1550|\n",
      "|55.676|12.571|        85| Diebold Nixdorf|          1|KÃƒÂ¸benhavn|Regnbuepladsen|            5|   1550|\n",
      "|55.676|12.571|        85| Diebold Nixdorf|          1|KÃƒÂ¸benhavn|Regnbuepladsen|            5|   1550|\n",
      "|55.676|12.571|        85| Diebold Nixdorf|          1|KÃƒÂ¸benhavn|Regnbuepladsen|            5|   1550|\n",
      "+------+------+----------+----------------+-----------+------------+--------------+-------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate data merge\n",
    "DIM_ATM.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unwanted columns\n",
    "columns_to_drop = ['location', 'streetname', 'street_number', \"zipcode\",\"lat\", \"lon\"]\n",
    "DIM_ATM = DIM_ATM.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atm_number', 'atm_manufacturer', 'location_id']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validate column drop\n",
    "DIM_ATM.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicates\n",
    "\n",
    "DIM_ATM = DIM_ATM.select(\"atm_number\",\"location_id\",\"atm_manufacturer\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atm_number', 'atm_location_id', 'atm_manufacturer']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Renaming columns according to target schema\n",
    "DIM_ATM = DIM_ATM.withColumnRenamed(\"atm_number\",\"atm_number\") \\\n",
    ".withColumnRenamed(\"atm_manufacturer\",\"atm_manufacturer\")\\\n",
    ".withColumnRenamed(\"location_id\",\"atm_location_id\")\n",
    "\n",
    "DIM_ATM.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+----------------+------+\n",
      "|atm_number|atm_location_id|atm_manufacturer|atm_id|\n",
      "+----------+---------------+----------------+------+\n",
      "|        95|              1|             NCR|     1|\n",
      "|        85|              1| Diebold Nixdorf|     2|\n",
      "|        85|              2| Diebold Nixdorf|     3|\n",
      "|        95|              2|             NCR|     4|\n",
      "|        47|              3|             NCR|     5|\n",
      "|        44|              4|             NCR|     6|\n",
      "|        46|              5| Diebold Nixdorf|     7|\n",
      "|        57|              6|             NCR|     8|\n",
      "|        86|              7|             NCR|     9|\n",
      "|         4|              8|             NCR|    10|\n",
      "+----------+---------------+----------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding primary key\n",
    "\n",
    "# Create the window specification\n",
    "w = Window.orderBy(\"atm_location_id\")\n",
    "\n",
    "# Use row number with the window specification\n",
    "DIM_ATM = DIM_ATM.withColumn(\"atm_id\", func.row_number().over(w))\n",
    "\n",
    "DIM_ATM.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- atm_id: integer (nullable = true)\n",
      " |-- atm_number: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rearranging columns according to target schema\n",
    "\n",
    "DIM_ATM = DIM_ATM.select('atm_id', 'atm_number', 'atm_manufacturer', 'atm_location_id')\n",
    "\n",
    "DIM_ATM.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validating ATM dimension count\n",
    "DIM_ATM.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Date <a id=\"date\"></a>\n",
    "\n",
    "- DIM_DATE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Column Name      Data Type\n",
    "-----------     ----------\n",
    "date_id        | INT\n",
    "full_date_time | TIMESTAMP\n",
    "year           | INT\n",
    "month          | VARCHAR(20)\n",
    "day            | INT\n",
    "hour           | INT\n",
    "weekday        | VARCHAR(20)\n",
    "\n",
    "PRIMARY KEY date_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8685"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get date dimension from master dataframe\n",
    "DIM_DATE = master_df.select('year', 'month', 'day', 'weekday', 'hour').distinct()\n",
    "DIM_DATE.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+--------+----+------------------+\n",
      "|year|  month|day| weekday|hour|    full_date_time|\n",
      "+----+-------+---+--------+----+------------------+\n",
      "|2017|January|  1|  Sunday|   9|  2017/January/1 9|\n",
      "|2017|January|  3| Tuesday|   5|  2017/January/3 5|\n",
      "|2017|January|  8|  Sunday|  19| 2017/January/8 19|\n",
      "|2017|January| 21|Saturday|   3| 2017/January/21 3|\n",
      "|2017|January| 23|  Monday|  21|2017/January/23 21|\n",
      "+----+-------+---+--------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "# Create full_date_time column\n",
    "\n",
    "concat(col(\"k\"), lit(\" \"), col(\"v\"))\n",
    "DIM_DATE = DIM_DATE.withColumn(\"full_date_time\", concat(col('year'), lit(\"/\"),col(\"month\"), lit(\"/\"),\n",
    "                                                        col('day'), lit(\" \"),col('hour')))\n",
    "DIM_DATE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- full_date_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "# Type casting full_date_time to timestamp\n",
    "date_Time_pattern = 'yyyy/MMMMM/dd H'\n",
    "DIM_DATE = DIM_DATE.withColumn('full_date_time', unix_timestamp(DIM_DATE['full_date_time'], date_Time_pattern).cast('timestamp'))\n",
    "DIM_DATE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+-------+----+-------------------+-------+\n",
      "|year|  month|day|weekday|hour|     full_date_time|date_id|\n",
      "+----+-------+---+-------+----+-------------------+-------+\n",
      "|2017|January|  1| Sunday|   0|2017-01-01 00:00:00|      1|\n",
      "|2017|January|  1| Sunday|   1|2017-01-01 01:00:00|      2|\n",
      "|2017|January|  1| Sunday|   2|2017-01-01 02:00:00|      3|\n",
      "|2017|January|  1| Sunday|   3|2017-01-01 03:00:00|      4|\n",
      "|2017|January|  1| Sunday|   4|2017-01-01 04:00:00|      5|\n",
      "+----+-------+---+-------+----+-------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding primary key\n",
    "\n",
    "# Create the window specification\n",
    "w = Window.orderBy(\"full_date_time\")\n",
    "\n",
    "# Use row number with the window specification\n",
    "DIM_DATE = DIM_DATE.withColumn(\"date_id\", func.row_number().over(w))\n",
    "\n",
    "DIM_DATE.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- full_date_time: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rearranging columns according to target schema\n",
    "\n",
    "DIM_DATE = DIM_DATE.select('date_id', 'full_date_time', 'year', 'month', 'day', 'hour', 'weekday')\n",
    "\n",
    "DIM_DATE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8685"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verify count\n",
    "DIM_DATE.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Card Type <a id=\"card\"></a>\n",
    "\n",
    "- DIM_CARD_TYPE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Column Name   Data Type\n",
    "-----------   -----------\n",
    "card_type_id  | INT\n",
    "card_type     | VARCHAR(20)\n",
    "\n",
    "\n",
    "PRIMARY KEY card_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           card_type|\n",
      "+--------------------+\n",
      "|     Dankort - on-us|\n",
      "|              CIRRUS|\n",
      "|         HÃƒÂ¦vekort|\n",
      "|                VISA|\n",
      "|  Mastercard - on-us|\n",
      "|             Maestro|\n",
      "|Visa Dankort - on-us|\n",
      "|        Visa Dankort|\n",
      "|            VisaPlus|\n",
      "|          MasterCard|\n",
      "|             Dankort|\n",
      "| HÃƒÂ¦vekort - on-us|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create card type dimension\n",
    "DIM_CARD_TYPE = master_df.select('card_type').distinct()\n",
    "DIM_CARD_TYPE.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|           card_type|card_type_id|\n",
      "+--------------------+------------+\n",
      "|     Dankort - on-us|           1|\n",
      "|              CIRRUS|           2|\n",
      "|         HÃƒÂ¦vekort|           3|\n",
      "|                VISA|           4|\n",
      "|  Mastercard - on-us|           5|\n",
      "|             Maestro|           6|\n",
      "|Visa Dankort - on-us|           7|\n",
      "|        Visa Dankort|           8|\n",
      "|            VisaPlus|           9|\n",
      "|          MasterCard|          10|\n",
      "|             Dankort|          11|\n",
      "| HÃƒÂ¦vekort - on-us|          12|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Add primary key\n",
    "pd_cardType = DIM_CARD_TYPE.toPandas()\n",
    "pd_cardType[\"card_type_id\"] = range(1, pd_cardType.shape[0]+1)\n",
    "DIM_CARD_TYPE = spark.createDataFrame(pd_cardType)\n",
    "DIM_CARD_TYPE.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- card_type_id: long (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rearranging columns according to target schema\n",
    "\n",
    "DIM_CARD_TYPE = DIM_CARD_TYPE.select('card_type_id', 'card_type')\n",
    "\n",
    "DIM_CARD_TYPE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verify count\n",
    "DIM_CARD_TYPE.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fact Table<a id=\"fact\"></a>\n",
    "\n",
    "- FACT_ATM_TRANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Location dimension\n",
    "FACT_ATM_TRANS = master_df.join(DIM_LOCATION,(master_df[\"atm_location\"] == DIM_LOCATION[\"location\"]) & \n",
    "                                (master_df[\"atm_streetname\"] == DIM_LOCATION[\"streetname\"]) &\n",
    "                                (master_df[\"atm_street_number\"] == DIM_LOCATION[\"street_number\"]) &\n",
    "                                (master_df[\"atm_zipcode\"] == DIM_LOCATION[\"zipcode\"]) &\n",
    "                                (master_df[\"atm_lat\"] == DIM_LOCATION[\"lat\"]) &\n",
    "                                (master_df[\"atm_lon\"] == DIM_LOCATION[\"lon\"]), how='inner')                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming to facilitate merg\n",
    "FACT_ATM_TRANS = FACT_ATM_TRANS.withColumnRenamed(\"atm_id\",\"atm_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join ATM dimension\n",
    "FACT_ATM_TRANS = FACT_ATM_TRANS.join(DIM_ATM,(FACT_ATM_TRANS[\"atm_number\"] == DIM_ATM[\"atm_number\"]) & \n",
    "                                (FACT_ATM_TRANS[\"atm_manufacturer\"] == DIM_ATM[\"atm_manufacturer\"]) &\n",
    "                                (FACT_ATM_TRANS[\"location_id\"] == DIM_ATM[\"atm_location_id\"]), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join CARD dimension\n",
    "FACT_ATM_TRANS = FACT_ATM_TRANS.join(DIM_CARD_TYPE, on = [\"card_type\"], how='inner') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Date dimension\n",
    "FACT_ATM_TRANS = FACT_ATM_TRANS.join(DIM_DATE,on = ['year', 'month', 'day', 'hour', 'weekday'], how='inner') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify count\n",
    "FACT_ATM_TRANS.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unwanted columns\n",
    "columns_to_drop = ['year', 'month', 'day', 'hour', 'weekday','atm_number', 'atm_manufacturer', 'atm_location', 'atm_streetname',\n",
    " 'atm_street_number', 'atm_zipcode', 'atm_lat', 'atm_lon','weather_lat', 'weather_lon', 'weather_city_id', 'weather_city_name',\n",
    " 'temp', 'pressure', 'humidity', 'wind_speed', 'wind_deg', 'card_type','location', 'streetname', 'street_number', 'zipcode',\n",
    " 'lat', 'lon','atm_number', 'atm_manufacturer', 'atm_location_id', 'full_date_time']\n",
    "FACT_ATM_TRANS = FACT_ATM_TRANS.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atm_status',\n",
       " 'currency',\n",
       " 'transaction_amount',\n",
       " 'service',\n",
       " 'message_code',\n",
       " 'message_text',\n",
       " 'rain_3h',\n",
       " 'clouds_all',\n",
       " 'weather_id',\n",
       " 'weather_main',\n",
       " 'weather_description',\n",
       " 'location_id',\n",
       " 'atm_id',\n",
       " 'card_type_id',\n",
       " 'date_id']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FACT_ATM_TRANS.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns according to target schema\n",
    "FACT_ATM_TRANS = FACT_ATM_TRANS.withColumnRenamed(\"location_id\",\"weather_loc_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+------------+-------+--------+\n",
      "|weather_loc_id|atm_id|card_type_id|date_id|trans_id|\n",
      "+--------------+------+------------+-------+--------+\n",
      "|            33|    50|           1|   6774|       1|\n",
      "|           109|   156|           1|    777|       2|\n",
      "|            11|    16|           1|   7570|       3|\n",
      "|            44|    66|           1|   5539|       4|\n",
      "|            64|    92|           1|   4278|       5|\n",
      "+--------------+------+------------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding primary key\n",
    "\n",
    "# Create the window specification\n",
    "w = Window.orderBy(\"transaction_amount\")\n",
    "\n",
    "# Use row number with the window specification\n",
    "FACT_ATM_TRANS = FACT_ATM_TRANS.withColumn(\"trans_id\", func.row_number().over(w))\n",
    "\n",
    "FACT_ATM_TRANS.select(\"weather_loc_id\",\"atm_id\",\"card_type_id\",\"date_id\",\"trans_id\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trans_id: integer (nullable = true)\n",
      " |-- atm_id: integer (nullable = true)\n",
      " |-- weather_loc_id: integer (nullable = true)\n",
      " |-- date_id: integer (nullable = true)\n",
      " |-- card_type_id: long (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- rain_3h: decimal(10,3) (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rearranging columns according to target schema\n",
    "\n",
    "FACT_ATM_TRANS = FACT_ATM_TRANS.select(\"trans_id\",\"atm_id\",\"weather_loc_id\",\"date_id\",\"card_type_id\",\"atm_status\",\n",
    "\"currency\",\"service\",\"transaction_amount\",\"message_code\",\"message_text\",\"rain_3h\",\"clouds_all\",\"weather_id\",\"weather_main\",\n",
    "\"weather_description\")\n",
    "\n",
    "FACT_ATM_TRANS.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data into s3 <a id=\"load\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - <u>S3 Bucket</u>: atmtransetl\n",
    " - <u>Folder</u>: atmtransetl/atm_trans\n",
    " - <u>Location dimension file path</u>: s3a://atmtransetl/atm_trans/location\n",
    " - <u>ATM dimension file path</u>: s3a://atmtransetl/atm_trans/atm\n",
    " - <u>Card dimension file path</u>: s3a://atmtransetl/atm_trans/card\n",
    " - <u>Date dimension file path</u>: s3a://atmtransetl/atm_trans/date\n",
    " - <u>Transactions fact file path</u>: s3a://atmtransetl/atm_trans/trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Writing Location dimension table into S3 bucket\n",
    "DIM_LOCATION.write.csv(\"s3a://atmtransetl/atm_trans/location\", mode=\"overwrite\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing ATM dimension table into S3 bucket\n",
    "DIM_ATM.write.csv(\"s3a://atmtransetl/atm_trans/atm\", mode=\"overwrite\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing card type dimension table into S3 bucket\n",
    "DIM_CARD_TYPE.write.csv(\"s3a://atmtransetl/atm_trans/card\", mode=\"overwrite\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing date dimension table into S3 bucket\n",
    "DIM_DATE.write.csv(\"s3a://atmtransetl/atm_trans/date\", mode=\"overwrite\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing Transaction fact table into S3 bucket\n",
    "FACT_ATM_TRANS.write.csv(\"s3a://atmtransetl/atm_trans/trans\", mode=\"overwrite\", header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
